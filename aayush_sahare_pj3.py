# -*- coding: utf-8 -*-
"""Aayush_Sahare_pj3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G2b2m51uztN6OmLmEEwotk2av5JNW4zn

Project 3: (10 points)
your total (50 points) will divided by 5 to get 10 points for this project.

In this project, we will focus on the preprocessing step before building the model.

We will prepare data before making a Market basket analysis, which is an algorithm originally designed to help retailers understand and improve their businesses.

Also, we will do some important preprocessing step before building an unsupervised model for Customer Segments.

### Market Basket Analysis
---
Imagine you work for a retailer that sells dozens of products and your boss comes to you and asks the following questions:

* What products are purchased together most frequently?
* How should the products be organized and positioned in the store?
* How do we identify the best products to discount via coupons?
---
##### You might reasonably respond with complete bewilderment, as those questions are very diverse and do not immediately seem answerable using a single algorithm and dataset. However, the answer to all those questions and many more is market basket analysis.
--
### Dataset:
you can find the dataset on Canvas under Dataset section: "Online Retail.xlsx"

### Steps to do:

1- Open a Jupyter notebook.

2- Install the following libraries, if not installed, and then import them:
* matplotlib.pyplot, which is used to plot the results of the models.
* mlxtend.frequent_patterns, which is used to run the models;
* mlxtend.preprocessing, which is used to encode and prep the data for the models;
* numpy, which is used to work with arrays;
* pandas, which is used to work with DataFrames.
"""

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

!pip install mlxtend

import matplotlib.pyplot as plt
import mlxtend.frequent_patterns
import mlxtend.preprocessing
import numpy
import pandas

"""3- Loading Data ( 1 point)

Load and view online retail dataset.Once you have downloaded the dataset, save it and note the path.
"""

# write your code here
import pandas as pd

# Load the dataset into a Pandas DataFrame
online_retail_data = pd.read_excel('Online Retail.xlsx')

# Display the first few rows of the dataset
online_retail_data.head()

"""4- Print out the first 10 rows of the DataFrame. ( 1 point)

Notice that the data contains some columns that will not be relevant to market basket analysis:
"""

# write your code here
online_retail_data.head(10)

"""5- Print out the data type for each column in the DataFrame. (1 point)

This information will come in handy when trying to perform specific cleaning tasks. Columns need to be of the correct type in order for filtering and computing to execute as expected:
"""

# write your code here
online_retail_data.dtypes

"""6- Get and print the dimensions of the DataFrame, as well as the number of unique invoice numbers and customer identifications (2 points)"""

# write your code here
# Get and print the dimensions of the DataFrame
data_dimensions = online_retail_data.shape
print("Dimensions of the DataFrame:", data_dimensions)

# Get and print the number of unique invoice numbers and customer identifications
unique_invoices = online_retail_data['InvoiceNo'].nunique()
unique_customers = online_retail_data['CustomerID'].nunique()

print("Number of unique invoice numbers:", unique_invoices)
print("Number of unique customer identifications:", unique_customers)

"""7- Data Cleaning and Formatting:

a. Create an indicator column stipulating whether the invoice number begins with "C". Called the column "IsCPresent" ( 1 points)
"""

# write your code here

# Creating the "IsCPresent" column

online_retail_data['IsCPresent'] = online_retail_data['InvoiceNo'].str.startswith('C')

# Display the updated DataFrame with the new column

online_retail_data.head()

"""b. (3 points)
- Filter out all transactions having either zero or a negative number of items (in other words, items were returned),
- Remove all invoice numbers starting with "C" using the column created in previous step
- Subset the DataFrame down to InvoiceNo and Description
- Drop all rows with at least one missing value.
- Rename the DataFrame online1 and print out the first 10 rows of the filtered DataFrame, online1.
"""

# write your code here

# Filter out transactions with zero or negative quantity
online_retail_data = online_retail_data[online_retail_data['Quantity'] > 0]

# Remove invoice numbers starting with "C" using the 'IsCPresent' column
online_retail_data = online_retail_data[~(online_retail_data['IsCPresent'].fillna(False))]

# Subset the DataFrame to InvoiceNo and Description
online1 = online_retail_data[['InvoiceNo', 'Description']]

# Drop rows with missing values
online1 = online1.dropna()

# Display the first 10 rows of the filtered DataFrame
online1.head(10)

"""c. Approximately, how many rows and invoice numbers have already removed? ( 2 points)

--
### Edit to write your answer here
"""

#write your code here to prove your answer:

# Original number of rows and unique invoice numbers
original_rows = online_retail_data.shape[0]
original_invoice_numbers = online_retail_data['InvoiceNo'].nunique()

# Filtered number of rows and unique invoice numbers
filtered_rows = online1.shape[0]
filtered_invoice_numbers = online1['InvoiceNo'].nunique()

# Print the results
print("Original Number of Rows:", original_rows)
print("Original Number of Unique Invoice Numbers:", original_invoice_numbers)
print("\nFiltered Number of Rows:", filtered_rows)
print("Filtered Number of Unique Invoice Numbers:", filtered_invoice_numbers)

# Approximate number of rows and invoice numbers removed
rows_removed = original_rows - filtered_rows
invoice_numbers_removed = original_invoice_numbers - filtered_invoice_numbers

print("\nApproximately,")
print("Rows Removed:", rows_removed)
print("Invoice Numbers Removed:", invoice_numbers_removed)

"""d. (2 points)
- Extract the invoice numbers from the DataFrame as a list.
- Remove duplicate elements to create a list of unique invoice numbers.
- Confirm that the process was successful by printing the length of the list of unique invoice numbers.
"""

# write your code here

# Extract invoice numbers from the DataFrame as a list
invoice_numbers_list = online1['InvoiceNo'].tolist()

# Remove duplicate elements to create a list of unique invoice numbers
unique_invoice_numbers = list(set(invoice_numbers_list))

# Confirm the success by printing the length of the list of unique invoice numbers
print("Length of Unique Invoice Numbers List:", len(unique_invoice_numbers))

"""e. Take the list from step d and cut it to only include the first 5,000 elements. Print out the length of the new list to confirm that it is, in fact, the expected length of 5,000( 1 point)"""

# write your code here
# Take the list of unique invoice numbers and cut it to include only the first 5,000 elements
limited_invoice_numbers = unique_invoice_numbers[:5000]

# Print out the length of the new list to confirm it is the expected length of 5,000
print("Length of Limited Invoice Numbers List:", len(limited_invoice_numbers))

"""f. Filter the online1 DataFrame down by only keeping the invoice numbers in the list from step e and print out the first 10 rows of online1( 1 point)"""

# write your code here
# Filter the online1 DataFrame to only include the invoice numbers in the list from step e
online1_filtered = online1[online1['InvoiceNo'].isin(limited_invoice_numbers)]

# Print out the first 10 rows of the filtered DataFrame
print(online1_filtered.head(10))

"""g. Print out the dimensions of the DataFrame (online1) and the number of unique invoice numbers to confirm that the filtering and cleaning process was successful (1 point)"""

# write your code here
# Print out the dimensions of the DataFrame (online1)
print("Dimensions of the DataFrame (online1):", online1_filtered.shape)

# Print out the number of unique invoice numbers in the filtered DataFrame
unique_invoice_numbers_filtered = online1_filtered['InvoiceNo'].nunique()
print("Number of Unique Invoice Numbers in the filtered DataFrame:", unique_invoice_numbers_filtered)

"""h. Transform the data in online1 into the aforementioned list of lists called invoice_item_list. The process for doing this is to iterate over the unique invoice numbers and, at each iteration, extract the item descriptions as a list and append that list to the larger invoice_item_list list. Print out elements one through four of the list (2 points)"""

# write your code here
# Initialize an empty list to store the lists of item descriptions
invoice_item_list = []

# Iterate over unique invoice numbers
for invoice_number in limited_invoice_numbers:

    # Extract item descriptions for the current invoice number
    items_for_invoice = online1_filtered.loc[online1_filtered['InvoiceNo'] == invoice_number, 'Description'].tolist()

    # Append the list of item descriptions to the larger invoice_item_list
    invoice_item_list.append(items_for_invoice)

# Print elements one through four of the list

print("Elements 1-4 of invoice_item_list:")
for i in range(4):
    print(invoice_item_list[i])

"""8- Data Encoding

While cleaning the data is crucial, the most important part of the data preparation process is molding the data into the correct form. Before running the models, the data, currently in the list of lists form, needs to be encoded and recast as a DataFrame.

To do this, we will leverage TransactionEncoder from the preprocessing module of mlxtend. The output from the encoder is a multidimensional array, where each row is the length of the total number of unique items in the transaction dataset and the elements are Boolean variables, indicating whether that particular item is linked to the invoice number that row represents. With the data encoded, we can recast it as a DataFrame where the rows are the invoice numbers and the columns are the unique items in the transaction dataset.

The data encoding will be done using mlxtend, but if you wish to encode the data without using a package, you are free.

More info about mlxtend in the link: https://rasbt.github.io/mlxtend/

a. Initialize and fit the transaction encoder. Print out an example of the resulting data. (2 points)
"""

# write your code here

from mlxtend.preprocessing import TransactionEncoder

# Initialize TransactionEncoder
te = TransactionEncoder()

# Fit the encoder on the list of lists (invoice_item_list)
te.fit(invoice_item_list)

# Transform the data using the fitted encoder
encoded_data = te.transform(invoice_item_list)

# Print an example of the resulting encoded data
print("Example of the resulting encoded data:")
print(encoded_data[:3])  # Print the first 3 rows as an example

"""b. Recast the encoded array as a DataFrame named online_encoder_df. Print the predefined subset of the DataFrame that features both True and False values (2 points)"""

# write your code here
import pandas as pd

# Recast the encoded array as a DataFrame
online_encoder_df = pd.DataFrame(encoded_data, columns=te.columns_)

# Print the predefined subset of the DataFrame that features both True and False values
print("Subset of online_encoder_df with True and False values:")


# Print the first 10 rows as an example
online_encoder_df.head(10)

"""c. Print out the dimensions of the encoded DataFrame. It should have 5,000 rows because the data used to generate it was previously filtered down to 5,000 unique invoice numbers. (1 point)"""

# write your code here
print("Dimensions of the encoded DataFrame (online_encoder_df):", online_encoder_df.shape)

"""#### The data is now prepared for modeling.
---

### Wholesale Data

we will analyze a dataset containing data on various customers' annual spending amounts (reported in monetary units) of diverse product categories for internal structure. One goal of this project is to best describe the variation in the different types of customers that a wholesale distributor interacts with. Doing so would equip the distributor with insight into how to best structure their delivery service to meet the needs of each customer.

--
### Dataset:
you can find the dataset on Canvas under Dataset section: "wholesale_customers_data.csv"
"""

import pandas as pd
import numpy as np
from numpy.linalg import inv
from numpy.random import uniform, multivariate_normal, rand, randn, seed
from itertools import repeat
from time import time
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import gridspec
from matplotlib.colors import to_rgba
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.neighbors import LocalOutlierFactor
from scipy.stats import jarque_bera, normaltest
import matplotlib.pyplot as plt
import scipy.stats as stats

"""1. Load the wholesale customers dataset (1 point)"""

# write your code here
wholesale_data = pd.read_csv('wholesale_customers_data.csv')

wholesale_data

"""2. Drop Channel & Region and print the current columns ( 1 point)"""

# write your code here
# Drop 'Channel' and 'Region' columns
wholesale_data = wholesale_data.drop(['Channel', 'Region'], axis=1)

# Print the current columns
print("Current Columns:")

wholesale_data.columns

"""### Data Exploration:

You will begin exploring the data through visualizations and code to understand how each feature is related to the others.

3. Show a statistical summary for each of the above product categories. (1 point)
"""

# write your code here
wholesale_data.describe()

"""4. Show a visual representation of the distribution of each feature in the data.using histogram and normal probability plot. (2 points)"""

# write your code here

# Loop through columns and create plots

# Set the style for seaborn
sns.set(style="whitegrid")

# Loop through each column and create histogram and normal probability plot
for column in wholesale_data.columns:
    # Create subplots
    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))

    # Plot Histogram
    sns.histplot(wholesale_data[column], kde=True, ax=axes[0])
    axes[0].set_title(f'Histogram of {column}')
    axes[0].set_xlabel(column)
    axes[0].set_ylabel('Frequency')

    # Plot Normal Probability Plot
    stats.probplot(wholesale_data[column], dist="norm", plot=axes[1])
    axes[1].set_title(f'Normal Probability Plot of {column}')
    axes[1].set_xlabel('Theoretical Quantiles')
    axes[1].set_ylabel('Ordered Values')

    # Adjust layout
    plt.tight_layout()

    # Show the plots
    plt.show()

"""5. What do you observe about distribution? (2 points)

The histogram and normal distribution plot indicate that the data distribution closely resembles a normal distribution. This suggests that most data points are concentrated near the mean, and outliers are minimal. Nevertheless, there are slight deviations from perfect normality, noticeable in a slight leftward skew.

The normal distribution plot demonstrates a generally good alignment of data points with theoretical quantiles, indicating a close resemblance to normal distribution. However, discrepancies between actual and theoretical quantiles suggest that the data distribution is not perfectly normal.

6. Scale the sample data using the natural logarithm ( 1 point)
"""

# Scale the data using the natural logarithm
# Apply natural logarithm to the entire DataFrame
wholesale_data_log = np.log1p(wholesale_data)

# Print the scaled data
print("Scaled Data using Natural Logarithm:")
print(wholesale_data_log.head())

"""7. Check for Outliers using boxplot ( 2 points)"""

# write your code here
# Set the style for seaborn
sns.set(style="whitegrid")

# Create boxplots for each feature
plt.figure(figsize=(12, 6))
sns.boxplot(data=wholesale_data_log, palette="Set2")
plt.title("Boxplots for Each Feature (Scaled using Natural Logarithm)")
plt.xlabel("Features")
plt.ylabel("Log-Scaled Values")
plt.show()

"""8. Apply natural log to transform long tails and plot the Log Sales Distribution using violinplot (2 points)"""

# write your code here
wholesale_data_log = np.log1p(wholesale_data)

# Melt the DataFrame for better visualization in a violin plot
wholesale_data_log_melted = wholesale_data_log.melt(var_name="Features", value_name="Log_Scaled_Values")

# Create a violin plot for the log-scaled distribution
plt.figure(figsize=(14, 8))
sns.violinplot(x="Features", y="Log_Scaled_Values", data=wholesale_data_log_melted, palette="viridis")
plt.title("Log Scaled Distribution (Violin Plot)")
plt.xlabel("Features")
plt.ylabel("Log Scaled Values")
plt.xticks(rotation=45, ha='right')
plt.show()

# write your code here

"""9. Remove Outliers using LocalOutlierFactor and plot the Log Sales Distribution using violinplot after removing the outliers. (2 points)"""

# write your code here
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.neighbors import LocalOutlierFactor

# Apply natural logarithm to the entire DataFrame
wholesale_data_log = np.log1p(wholesale_data)

# Use Local Outlier Factor to detect and remove outliers
lof = LocalOutlierFactor(contamination=0.05)  # Adjust contamination as needed
outliers = lof.fit_predict(wholesale_data_log)

# Filter out rows that are not outliers
wholesale_data_no_outliers = wholesale_data_log[outliers != -1]

# Melt the DataFrame for better visualization in a violin plot
wholesale_data_no_outliers_melted = wholesale_data_no_outliers.melt(var_name="Features", value_name="Log_Scaled_Values")

# Create a violin plot for the log-scaled distribution after removing outliers
plt.figure(figsize=(14, 8))
sns.violinplot(x="Features", y="Log_Scaled_Values", data=wholesale_data_no_outliers_melted, palette="viridis")
plt.title("Log Scaled Distribution After Outlier Removal (Violin Plot)")
plt.xlabel("Features")
plt.ylabel("Log Scaled Values")
plt.xticks(rotation=45, ha='right')
plt.show()

"""10. use sns.pairplot to visualize Feature Distributions in your cleaned data (2 points)"""

# write your code here
# Melt the DataFrame for pairplot
wholesale_data_melted = wholesale_data_no_outliers.melt()

# Create a pairplot
sns.set(style="ticks")
pair_plot = sns.pairplot(wholesale_data_melted, hue="variable", diag_kind="kde", palette="viridis", height=3)

# Show the plot
plt.show()

"""11. Check for Correlations using sns.clustermap (2 points)"""

# write your code here

# Calculate the correlation matrix
correlation_matrix = wholesale_data_no_outliers.corr()

# Create a clustered heatmap
plt.figure(figsize=(12, 10))
sns.clustermap(correlation_matrix, cmap="coolwarm", annot=True, linewidths=.5)
plt.title("Correlation Heatmap")
plt.show()

"""12. What do you observe? (2 points)

Insights from the Heatmap:

Significant correlations are observed between certain items, such as:

Milk and Groceries.
Detergent Papers and Grocery.
This finding suggests a potential strategy for the retail industry and supermarkets to optimize stocking practices by placing these items together. Customers are more inclined to purchase them in tandem.

Conversely, there is a notably weak correlation between:

Frozen items and Detergent Papers.
Grocery and Frozen items.
This implies that grouping these items together might not significantly impact overall sales and should be carefully considered in stocking decisions.

### PCA
Now that the data has been scaled to a more normal distribution and has had any necessary outliers removed, we can now apply PCA to the cleaned data to discover which dimensions about the data best maximize the variance of features involved.

13. Biplot: Visualizing Product Relationships in 2D (2 points)
"""

# write your code here
# Apply PCA to the cleaned data
pca = PCA(n_components=2)
pca_result = pca.fit_transform(wholesale_data_no_outliers)

# Create a DataFrame with the PCA results
pca_df = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])

# Plot the biplot
plt.figure(figsize=(10, 8))
sns.scatterplot(x='PC1', y='PC2', data=pca_df)

# Add variable loadings to the biplot
for i, feature in enumerate(wholesale_data_no_outliers.columns):
    plt.arrow(0, 0, pca.components_[0, i], pca.components_[1, i], color='r', alpha=0.5)
    plt.text(pca.components_[0, i]*1.2, pca.components_[1, i]*1.2, feature, color='r', ha='center', va='center')

plt.title('Biplot: Product Relationships in 2D')
plt.xlabel('Principal Component 1 (PC1)')
plt.ylabel('Principal Component 2 (PC2)')
plt.grid(True)
plt.show()

"""14. Use sns.jointplot to plot x and y in the above code (2 points)"""

# write your code here
# Apply PCA to the cleaned data
pca = PCA(n_components=2)
pca_result = pca.fit_transform(wholesale_data_no_outliers)

# Create a DataFrame with the PCA results
pca_df = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])

# Use sns.jointplot to visualize the relationship
sns.jointplot(x='PC1', y='PC2', data=pca_df, kind='scatter', height=8)
plt.suptitle('Jointplot: Relationship between PC1 and PC2')
plt.show()

"""15. Exploring the new Descriptors of the cleand Data using plot.bar (2 points)"""

# write your code here
# Apply PCA to the cleaned data
pca = PCA(n_components=2)
pca_result = pca.fit_transform(wholesale_data_no_outliers)

# Create a DataFrame with the PCA results
pca_df = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])

# Plot bar plot for the first two principal components
plt.figure(figsize=(10, 6))
pca_df.mean().plot.bar(color='skyblue')
plt.title('Mean Values of Principal Components (PC1 and PC2)')
plt.xlabel('Principal Components')
plt.ylabel('Mean Value')
plt.show()

"""16. How much approximately the first and second features of explain of variance in total? (1 point)


"""

# Apply PCA to the cleaned data
pca = PCA(n_components=2)
pca_result = pca.fit_transform(wholesale_data_no_outliers)

# Explained variance ratios for each principal component
explained_variance_ratio = pca.explained_variance_ratio_

# Print the explained variance for the first and second features
print(f"Explained Variance for PC1: {explained_variance_ratio[0]:.2%}")
print(f"Explained Variance for PC2: {explained_variance_ratio[1]:.2%}")

"""---
## All set

Please make sure you execute each cell before you submit your file; this is important because if your code didn't work on our machine for an unknown reason, it would be better to see the result rather than losing some points or wasting time to contact you to fix this issue.

What to submit:

* Your Jupyter Notebook file.
* Name your file as firstname_lastname_pj_3 .
---
"""